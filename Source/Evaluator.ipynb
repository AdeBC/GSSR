{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 性能指标的计算公式\n",
    "\n",
    "$Sensitivity(Recall) = \\frac{TP}{TP+FN}$\n",
    "\n",
    "$Specificity = \\frac{TN}{TN+FP}$\n",
    "\n",
    "$Tpr = \\frac{TP}{TP+FN}$\n",
    "\n",
    "$Fpr = \\frac{FP}{FP+TN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \n",
    "    def __init__(self, **kwgs):\n",
    "        self.score_names = ['wam_scores', 'bn_scores', 'svm_scores']\n",
    "        self.Multi_scores = {key: kwgs[key] for key in self.score_names}\n",
    "        self.labels = kwgs['labels']\n",
    "        self.Conf_mas = []\n",
    "        \n",
    "    def Sn_Sp_Curves(self, T_range=np.arange(0, 10, 0.5) ):\n",
    "        if self.Conf_mas == []:\n",
    "            Conf_tbs = self.Confusion_table(T_range)\n",
    "        # use plt to plot sn-sp-curve\n",
    "        lw = 2\n",
    "        #plt.figure(figsize=(5,5), dpi=120)\n",
    "\n",
    "        for name, tb in Conf_tbs.items():\n",
    "            table = self.Cal_Sn_Sp(tb)\n",
    "            # use plt to plot ROC\n",
    "            Sn = table['Sensitivity']\n",
    "            Sp = table['Specificity']\n",
    "            name = name.rstrip('_scores').upper()#.ljust(5, ' ')\n",
    "            \n",
    "            plt.plot(Sp, Sn, \n",
    "                     lw=lw, label='Sn-Sp curve for {}'.format(name)) \n",
    "        plt.plot([1, 0], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Specificity')\n",
    "        plt.ylabel('Sensitivity')\n",
    "        plt.title('Sn-Sp Curve')\n",
    "        plt.legend(loc=\"lower left\", fontsize=8)\n",
    "        #plt.show()\n",
    "        \n",
    "    def ROC_Curves(self, T_range=np.arange(0, 10, 0.5) ):\n",
    "        if self.Conf_mas == []:\n",
    "            Conf_tbs = self.Confusion_table(T_range)\n",
    "        lw = 2\n",
    "        #plt.figure(figsize=(5,5), dpi=120)\n",
    "\n",
    "        for name, tb in Conf_tbs.items():\n",
    "            table = self.Cal_Tpr_Fpr(tb)\n",
    "            # use plt to plot ROC\n",
    "            Tpr = table['Tpr']\n",
    "            Fpr = table['Fpr']\n",
    "            roc_auc = auc(Fpr,Tpr)\n",
    "            name = name.rstrip('_scores').upper()#.ljust(5, ' ')\n",
    "            \n",
    "            plt.plot(Fpr, Tpr, \n",
    "                     lw=lw, label='ROC curve for {} (area = {:.2})'.format(name, roc_auc)) ###假正率为横坐标，真正率为纵坐标做曲线\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic')\n",
    "        plt.legend(loc=\"lower right\", fontsize=8)\n",
    "        #plt.show()\n",
    "\n",
    "    def Confusion_table(self, T_range):\n",
    "        y_true = self.labels\n",
    "        mScores = self.Multi_scores\n",
    "        names = self.score_names\n",
    "        ys_pred = { name: pd.DataFrame({ T: (mScores[name] > T).astype(np.int) for T in T_range }) \n",
    "                   for name in names }\n",
    "        stats_Multi_T = { name: pd.DataFrame({'Threshold': [T for T in T_range]}) for name in names }\n",
    "        for n in names:\n",
    "            # tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
    "            stats_tmp = stats_Multi_T[n]['Threshold'].apply( lambda T: confusion_matrix( y_true, ys_pred[n][T], labels=[0,1] ).ravel() )\n",
    "            stats_tmp = [stat for stat in zip(*stats_tmp)]\n",
    "\n",
    "            stats_Multi_T[n]['TN'] = stats_tmp[0]\n",
    "            stats_Multi_T[n]['FP'] = stats_tmp[1]\n",
    "            stats_Multi_T[n]['FN'] = stats_tmp[2]\n",
    "            stats_Multi_T[n]['TP'] = stats_tmp[3]\n",
    "        return stats_Multi_T\n",
    "        \n",
    "    def Cal_Sn_Sp(self, conf_tb):\n",
    "        conf_tb['Sensitivity'] = conf_tb.apply(lambda x: x['TP'] / (x['TP']+x['FN']), axis=1)\n",
    "        conf_tb['Specificity'] = conf_tb.apply(lambda x: x['TN'] / (x['TN']+x['FP']), axis=1)\n",
    "        return conf_tb[['Threshold', 'Sensitivity', 'Specificity']]\n",
    "    \n",
    "    def Cal_Tpr_Fpr(self, conf_tb):\n",
    "        conf_tb['Tpr'] = conf_tb.apply(lambda x: x['TP'] / (x['TP']+x['FN']), axis=1)\n",
    "        conf_tb['Fpr'] = conf_tb.apply(lambda x: x['FP'] / (x['TN']+x['FP']), axis=1)\n",
    "        return conf_tb[['Threshold', 'Tpr', 'Fpr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
